# LLM Label Validation Action Plan

## Overview

This document outlines a systematic approach to validate and cross-check LLM-generated labels for a dataset of 6,000 sentences. The goal is to ensure label quality and consistency through human review, automated model comparison, and iterative refinement.

## Context

- **Dataset Size**: ~6,000 sentences
- **Labels**: Generated by LLM
- **Timeline**: 1 week (compressed schedule)
- **Objective**: Validate label accuracy and consistency
- **Approach**: Multi-layered validation combining human review, automated comparison, and refinement (running in parallel)

---

## Phase 1: Random Sampling for Human Review

### Objective
Establish a baseline quality metric by having human reviewers validate a representative sample of LLM-generated labels.

### Steps

#### 1.1 Determine Sample Size
- **For 1-week timeline**: 150-200 sentences (~2.5-3% of total dataset)
- **Rationale**:
  - Manageable for quick human review (can be completed in 1-2 days)
  - Still provides statistical validity (~90% confidence level with ±8% margin)
  - Allows time for model comparison and refinement in same week

#### 1.2 Select Random Sample
- **Method**: Use stratified random sampling if labels have different categories
  - Ensure proportional representation of all label types
  - Use random seed for reproducibility

- **Tools/Scripts**:
  ```python
  # Example sampling approach
  import pandas as pd
  import numpy as np

  # Set seed for reproducibility
  np.random.seed(42)

  # Load dataset
  df = pd.read_csv('labeled_sentences.csv')

  # Stratified sampling (if applicable)
  sample_df = df.groupby('label', group_keys=False).apply(
      lambda x: x.sample(frac=0.05, random_state=42)
  )

  # Or simple random sampling
  # sample_df = df.sample(n=250, random_state=42)

  sample_df.to_csv('validation_sample.csv', index=False)
  ```

#### 1.3 Prepare Review Guidelines
Create clear annotation guidelines for human reviewers:
- Define each label category clearly
- Provide examples of correct labeling
- Include edge cases and how to handle them
- Create a simple review interface/spreadsheet

#### 1.4 Conduct Human Review
- **Reviewers**: 2-3 human annotators (for inter-annotator agreement)
- **Format**: Spreadsheet or annotation tool
- **Required Fields**:
  - Sentence ID
  - Original sentence
  - LLM-assigned label
  - Human-assigned label
  - Agreement (Yes/No)
  - Notes/Comments (for disagreements)

#### 1.5 Calculate Metrics
- **Agreement Rate**: Percentage where human agrees with LLM
- **Cohen's Kappa**: Measure inter-rater reliability if using multiple reviewers
- **Confusion Matrix**: Identify systematic labeling errors
- **Target**: Aim for >90% agreement rate

### Deliverables (1-Week Version)
- [ ] Sample dataset (150-200 sentences)
- [ ] Simple annotation guidelines (1-page)
- [ ] Completed human review spreadsheet
- [ ] Baseline metrics report (can be brief)

---

## Phase 2: Use Another Model for Comparison

### Objective
Automated secondary validation using a different model to identify potential labeling issues at scale.

### Steps

#### 2.1 Select Comparison Model
Choose a second model for validation:

**Option A: Different Model Architecture**
- Use a different fine-tuned RoBERTa checkpoint
- Try a different model family (BERT, DistilBERT, ALBERT)
- Consider a larger/smaller model variant

**Option B: Alternative Approach**
- Use zero-shot classification (e.g., BART, T5)
- Try few-shot learning with GPT-3.5/GPT-4
- Use rule-based heuristics as sanity check

**Recommendation**: Start with a different RoBERTa checkpoint or BERT-based model for consistency

#### 2.2 Run Comparison on Sample
- Run the second model on the same 200-300 sentence sample
- Compare outputs between:
  - Original LLM labels
  - Human review labels (from Phase 1)
  - Second model labels

#### 2.3 Analyze Agreement Patterns
Calculate agreement metrics:
```python
# Calculate three-way agreement
from sklearn.metrics import cohen_kappa_score, classification_report

# Model 1 vs Model 2
model_agreement = (labels_model1 == labels_model2).mean()

# Model 1 vs Human
human_model1_kappa = cohen_kappa_score(labels_human, labels_model1)

# Model 2 vs Human
human_model2_kappa = cohen_kappa_score(labels_human, labels_model2)

# Identify high-confidence agreements (both models agree)
high_confidence = (labels_model1 == labels_model2)

# Identify discrepancies needing review
discrepancies = (labels_model1 != labels_model2)
```

#### 2.4 Scale to Full Dataset (Optional)
If computational resources allow:
- Run second model on entire 6,000 sentence dataset
- Flag sentences where models disagree for manual review
- Prioritize reviewing high-disagreement cases

### Deliverables
- [ ] Second model selection rationale document
- [ ] Comparison model predictions on sample
- [ ] Agreement analysis report
- [ ] List of flagged discrepancies

---

## Phase 3: Spot-Check Discrepancies and Adjust

### Objective
Investigate disagreements, identify patterns, and refine the labeling process.

### Steps

#### 3.1 Categorize Discrepancies
Group disagreements by type:
- **Type A**: Human disagrees with both models
- **Type B**: Models agree but human disagrees
- **Type C**: Human agrees with Model 2 but not Model 1
- **Type D**: Edge cases/ambiguous sentences

#### 3.2 Deep-Dive Analysis
For each discrepancy type:
1. **Identify Patterns**
   - Are certain label categories more problematic?
   - Do errors occur with specific sentence structures?
   - Are there domain-specific terms causing confusion?

2. **Root Cause Analysis**
   - Model limitations (training data, architecture)
   - Ambiguous label definitions
   - Edge cases not covered in guidelines
   - Data quality issues (typos, formatting)

#### 3.3 Refine Labeling Approach
Based on findings:

**Option 1: Update Label Definitions**
- Clarify ambiguous category definitions
- Add examples for edge cases
- Update annotation guidelines

**Option 2: Re-label Problematic Categories**
- Identify categories with <80% agreement
- Re-run LLM with improved prompts
- Add few-shot examples to prompts

**Option 3: Model Fine-tuning**
- Create gold-standard training set from human-validated samples
- Fine-tune model on corrected labels
- Re-evaluate on held-out test set

**Option 4: Hybrid Approach**
- Use high-confidence automated labels (both models agree)
- Flag low-confidence cases for human review
- Implement confidence thresholds

#### 3.4 Implement Corrections
1. Create correction pipeline
2. Document decisions and rationale
3. Update affected labels in main dataset
4. Re-validate corrected samples

#### 3.5 Final Validation Round (Compressed)
- **For 1-week timeline**: Spot-check 30-50 sentences only
  - Include mix of original problematic cases and new samples
  - Quick review by 1-2 people (2-3 hours max)
  - Verify improvements in top discrepancy categories
- **Target**: >85% agreement on spot-check (realistic for 1 week)
- **Note**: Full re-validation can be done later if needed

### Deliverables
- [ ] Discrepancy analysis report with categorization
- [ ] Pattern identification document
- [ ] Refined labeling guidelines/prompts
- [ ] Updated dataset with corrections
- [ ] Final validation results

---

## Implementation Timeline (1-Week Compressed Schedule)

**Key Strategy**: Run phases in parallel to compress timeline from 4-6 weeks to 1 week

| Day | Tasks | Phase | Deliverables |
|-----|-------|-------|--------------|
| **Day 1 (Mon)** | - Generate sample (150-200 sentences)<br>- Set up review spreadsheet<br>- Start model comparison setup | Setup | Sample CSV, Review template |
| **Day 2 (Tue)** | - Begin human review (parallel reviewers)<br>- Run second model on full 6K dataset | Phase 1 & 2 | 50% review complete, Model running |
| **Day 3 (Wed)** | - Complete human review<br>- Analyze model comparison results<br>- Calculate initial metrics | Phase 1 & 2 | Completed reviews, Agreement metrics |
| **Day 4 (Thu)** | - Identify discrepancies<br>- Categorize disagreements<br>- Pattern analysis | Phase 3 | Discrepancy report |
| **Day 5 (Fri)** | - Quick refinement (update prompts/guidelines)<br>- Spot-check fixes on 30-50 samples<br>- Final validation | Phase 3 | Updated labels, Final report |

**Total Duration**: 5 business days (1 week)

### Parallel Processing Strategy

To achieve 1-week timeline:
1. **Day 1-2**: Start model comparison while human review is ongoing
2. **Day 2-3**: Multiple reviewers work in parallel (split the 150-200 samples)
3. **Day 3-4**: Analyze results from both phases simultaneously
4. **Day 4-5**: Rapid iteration on highest-impact issues only

---

## Resources Required

### Human Resources (1-Week Timeline)
- **2-3 annotators** for parallel review (4-6 hours total each, split across Days 2-3)
  - Each reviews 50-75 sentences
  - Work simultaneously to finish in 2 days
- **1 ML engineer** for model setup and comparison (Days 1-3)
- **1 data scientist** for analysis and refinement (Days 3-5)
- **Note**: Can be done with 1 person wearing multiple hats if needed

### Technical Resources
- GPU access for running second model (if needed)
- Annotation tool or spreadsheet software
- Python environment with scikit-learn, pandas, transformers

### Data Requirements
- Original 6,000 labeled sentences
- Metadata (if available): label confidence scores, timestamps
- Label schema documentation

---

## Success Criteria (1-Week Timeline)

| Metric | Target | Stretch Goal | Current | Status |
|--------|--------|--------------|---------|--------|
| Human-LLM Agreement (sample) | >80% | >90% | TBD | Pending |
| Model-Model Agreement | >75% | >85% | TBD | Pending |
| Cohen's Kappa | >0.70 | >0.80 | TBD | Pending |
| Spot-Check Validation (post-fix) | >85% | >90% | TBD | Pending |
| Identified Patterns | 3-5 key issues | 5-10 issues | TBD | Pending |

**Note**: Targets are adjusted for compressed 1-week timeline. Higher accuracy can be achieved with additional iteration time.

---

## Trade-offs for 1-Week Timeline

### What We're Optimizing For:
- ✅ **Speed**: Get actionable insights within 5 days
- ✅ **Practical validation**: Identify top 3-5 issues quickly
- ✅ **Quick wins**: Fix highest-impact problems
- ✅ **Baseline metrics**: Establish initial quality measurements

### What We're Trading Off:
- ⚠️ **Smaller sample size**: 150-200 vs 300 sentences (but still statistically valid)
- ⚠️ **Less iteration**: 1 refinement cycle instead of 2-3
- ⚠️ **Narrower focus**: Address top issues only, not edge cases
- ⚠️ **Lower final accuracy**: 85-90% vs 95%+ (can improve later)

### Recommended Follow-up (Post-Week 1):
If initial validation reveals issues:
1. **Week 2+**: Expand validation to 500+ sentences
2. **Ongoing**: Implement continuous monitoring
3. **Monthly**: Re-validate on new samples

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|-----------|
| Low initial agreement (<80%) | High | Expand sample size, conduct deeper analysis |
| Inconsistent human reviewers | Medium | Use 2-3 reviewers, calculate inter-annotator agreement |
| Computational constraints | Medium | Prioritize sample validation over full dataset |
| Ambiguous label definitions | High | Refine guidelines iteratively, add examples |

---

## Next Steps (1-Week Sprint)

### Day 1 - Setup (Monday)
- [ ] Generate random sample of 150-200 sentences
- [ ] Create simple annotation guidelines (1-page max)
- [ ] Set up Google Sheets review template
- [ ] Identify/recruit 2-3 reviewers
- [ ] Select second model and prepare environment
- [ ] **Time commitment**: 4-6 hours

### Day 2-3 - Parallel Execution (Tuesday-Wednesday)
- [ ] Human review in progress (parallel reviewers)
- [ ] Run second model on sample + full dataset
- [ ] Daily check-in on review progress
- [ ] Calculate preliminary metrics as reviews come in
- [ ] **Time commitment**: 8-10 hours total

### Day 4 - Analysis (Thursday)
- [ ] Complete all metrics calculations
- [ ] Identify top 3-5 discrepancy patterns
- [ ] Categorize issues by severity
- [ ] Draft quick fixes (updated prompts/guidelines)
- [ ] **Time commitment**: 6-8 hours

### Day 5 - Refinement & Wrap-up (Friday)
- [ ] Implement highest-impact fixes
- [ ] Spot-check 30-50 corrected samples
- [ ] Document findings and recommendations
- [ ] Deliver final validation report
- [ ] **Time commitment**: 4-6 hours

---

## Documentation

All findings, decisions, and changes should be documented:

- **Review Log**: Track all human review sessions
- **Decision Log**: Record all labeling decisions and rationale
- **Change Log**: Document all label corrections
- **Metrics Dashboard**: Track agreement metrics over time

---

## References & Resources

- Inter-Annotator Agreement: [Cohen's Kappa Explanation](https://en.wikipedia.org/wiki/Cohen%27s_kappa)
- Labeling Best Practices: [Google's Data Labeling Guide](https://cloud.google.com/ai-platform/data-labeling/docs/best-practices)
- Model Evaluation: [scikit-learn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)

---

**Document Version**: 2.0 (1-Week Compressed Timeline)
**Last Updated**: 2025-10-22
**Status**: Active
**Timeline**: 5 business days (1 week)

